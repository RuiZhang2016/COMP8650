\message{ !name(ass3_ChangLi.tex)}\documentclass[10pt,a4paper]{article}
\usepackage{amssymb,amsmath}

\begin{document}

\message{ !name(ass3_ChangLi.tex) !offset(-3) }

\title{Ass3 Solutions}
\author{Chang Li}
\maketitle

\section{Solutions}

\subsection{Prob 4.3}
Because $\nabla f(x) = \frac{1}{2}(P+P^T)x + q$,

\begin{align*}
  \nabla f(x^*) &=\frac{1}{2}(
                  \begin{bmatrix}
                    13 & 12 & -2\\
                    12 & 17 & 6\\
                    -2 & 6 & 12
                  \end{bmatrix}
                             +
                             \begin{bmatrix}
                               13 & 12 & -2\\
                               12 & 17 & 6\\
                               -2 & 6 & 12
                             \end{bmatrix})
                                        \begin{bmatrix}
                                          1\\
                                          \frac{1}{2}\\
                                          -1
                                        \end{bmatrix} +
  \begin{bmatrix}
    -22\\
    -14.5\\
    13
  \end{bmatrix}\\
                &=
                  \begin{bmatrix}
                    13 & 12 & -2\\
                    12 & 17 & 6\\
                    -2 & 6 & 12
                  \end{bmatrix}
                             \begin{bmatrix}
                               1\\
                               \frac{1}{2}\\
                               -1
                             \end{bmatrix} +
  \begin{bmatrix}
    -22\\
    -14.5\\
    13
  \end{bmatrix}\\
                &=
                  \begin{bmatrix}
                    -1\\
                    0\\
                    2
                  \end{bmatrix}
\end{align*}
According to equation (4.21)\cite{boyd2004convex}, because for all
$y$ satisfying $y_i \in [-1,1], \; i=1,2,3$, the optimality
condition holds:
\begin{align*}
 \nabla f(x^*)^T(y-x^*)&=\begin{bmatrix}
-1 & 0 & 2
\end{bmatrix}
\begin{bmatrix}
  y_1 - 1\\
  y_2 - 1/2\\
  y_3 + 1
\end{bmatrix}\\
&=
-1(y_1-1)+2(y_3+1)\geq0
\end{align*}
therefore, $x^*$ is optimal for $f(x)$.


\subsection{Prob 4.8}

\subsubsection{a}
The Lagrangian of the primal problem is:
\begin{align*}
  L(x,v) &= c^Tx + v^T(Ax-b)\\
         &= -b^Tv+(c+A^Tv)x
\end{align*}
$L$ is bounded below if and only if $c+A^Tv=0$. Therefore, the
primal problem's optimal value $p^*=-b^Tv$ for some $v$
satisfying $c+A^Tv=0$. The primal problem is unbounded otherwise.

\subsubsection{b}
The problem is always feasible. The Lagrangian of the primal
problem is:
\begin{align*}
  L(x,v) &= c^Tx + \lambda(a^Tx-b)\\
         &= -b\lambda+(c+a^T\lambda)x
\end{align*}
$L$ is bounded below if and only if $c+a^T\lambda=0$ and
$\lambda\geq0$. Therefore, the primal problem has optimal value
$p^*=-b\lambda$ for some $\lambda$ satisfying $c+a^T\lambda=0$ and
$\lambda\geq0$. Otherwise the primal problem is unbounded below.

\subsubsection{c}

The objective function can be written as
$\sum_{i=1}^nc_ix_i$. Therefore, the objective function
reaches minimal when each of component $c_ix_i$ is
minimized subject to $l_i\leq x_i\leq u_i$.

Therefore, for all indices such that $c_i>0$ we have
$x_i=l_i$ and for all indices such that $c_i<0$ we have
$x_i=u_i$. When $c_i=0$, $x_i$ can be any value in the
domain of the problem namely $l_i\leq x_i\leq u_i$.

\subsubsection{d}

The Lagrangian of the primal problem is:
\begin{align*}
  L(x,v,\lambda) &= c^Tx + v(1^Tx-1)-\lambda^Tx\\
         &= -v+(c^T+v-\lambda^T)x
\end{align*}

$L$ is bounded below if and only if $c^T+v-\lambda^T=0$ and
$\lambda\succeq0$, namely $-v< c_i \;\forall i$. Therefore,
the primal problem has optimal value $p^*\geq -v$ for some
$\lambda$ satisfying $\lambda\succeq0$ and for some $v$
satisfying $-v <c_i\;\forall i$. Suppose $c$ has minimal
values $c_{min}$ at indexes $i,j,k\dots$, we can always get
the optimal solution by setting $x_i, x_j, x_k,\dots$ to
$\sum_{m=i,j,k\dots}x_m=1$ at those indexes and $0$ at
anywhere else. Therefore, we have $p^*=c_{min}$. Otherwise
the primal problem is unbounded below.

When we use inequality constraint to replace the equality
constraint, the optimal value is equal to
$p^*=min\{0,c_{min}\}$. This is because we can assign $x$ as
above if $c_{min}<0$ and $x=0$ otherwise.

\subsubsection{e}
Suppose the components of $c$ are sorted in increasing order
as:
$$
c_1\leq c_2\leq \dotsb \leq c_{\alpha} \leq \dotsb \leq c_n
$$
When $\alpha$ is an integer, the primal problem has optimal
value $p^*=\sum_{i=1}^\alpha c_i$, namely the smallest
$\alpha$ elements of $c$ if and only if $x_i=1
\text{\;for\;} i\leq \alpha$ and $x_i=0 \text{\; for \;}
i>\alpha$.

If $\alpha$ is not an integer, let $[\alpha]$ denotes the
nearest integer less than or equal to $\alpha$, the optimal value is
$$
p^*=\sum_{i=1}^{[\alpha]}c_i+c_{1+[\alpha]}(\alpha-[\alpha])
$$
by setting $x_i=1$ for $i\leq [\alpha]$, 
$x_{1+[\alpha]}=\alpha-[\alpha]$ and $x_i=0$ for $i>1+[\alpha]$.

If change the equality constraint to inequality constraint
$1^Tx\leq \alpha$, then the optimal value is 
\begin{align*}
  p^*=
  \begin{cases}
    \sum_{i=1}^{[\alpha]}c_i+c_{1+[\alpha]}(\alpha-[\alpha]) & if c_{1+[\alpha]} <0\\
    \sum_{i=1}^j c_i & if c_j+1\geq =0 \text{\;and\;} j<1+[\alpha]\\
    0 & if c_1>=0
  \end{cases}
\end{align*}
We can always get this by setting:
\begin{align*}
  x_i=
  \begin{cases}
    0 & c_i\geq0\\
    1 & c_i<0
  \end{cases}
\text{\;for\;} i\leq [\alpha]
\end{align*}
\begin{align*}
x_{1+[\alpha]}=
  \begin{cases}
    \alpha-[\alpha] & if c_{1+[\alpha]}<0\\
    0 &\text{otherwise}
  \end{cases}
\end{align*}
 and $x_i=0$ for $i>[\alpha]+1$.

\subsubsection{f}
Let $y_i=d_ix_i$ for $i=1,\dots,n$ and use $y$ to substitute
$x$ in the original problem. Then we have:
$$
\text{minimize} \sum_{i=1}^n \frac{c_i}{d_i}y_i
$$
$$
\text{s.t.\;} 1^Ty=\alpha\text{,\;\;} 0\preceq y \preceq d.
$$
This formulation is similar to section 1.2.5. Suppose the
components of $\frac{c_i}{d_i}$ are sorted in increasing
order as:
$$
\frac{c_1}{d_1}\leq \dotsb \leq \frac{c_\alpha}{d_\alpha} \leq \dotsb \leq\frac{c_n}{d_n} 
$$
Let $k^*=\text{argmax}_kd_1+\dots+d_k\leq \alpha$, the primal problem reaches
optimal value
$p^*=\sum_{i=1}^{k^*}c_i+\frac{c_{k^*+1}}{d_{k^*+1}}(\alpha-(d_1+\dots+d_{k^*}))$
by setting $y_i=d_i$ for $i\leq k^*$, $y_{k^*+1}=\alpha-(d_1+\dots+d_{k^*})$ and
$y_i=0$ for $i>k^*+1$.

Namely by setting $x_i=1$ for $i\leq k^*$, $x_{k^*+1}=\frac{\alpha-(d_1+\dots+d_{k^*})}{d_{k+1}}$ and
$x_i=0$ for $i>k^*+1$.


\subsection{Prob 4.9}
Because $A$ is nonsingular and square, we have $A^{-1}$. Substituting
$y=Ax$ into the primal problem we have:
$$
\text{minimize\;} c^TA^{-1}y
$$
$$
\text{s.t.\;\;} y\preceq b
$$
The primal problem is unbounded below if $c^TA^{-1}\succ0$.
When $c^TA^{-1}\preceq 0$ we have optimal value
$p^*=c^TA^{-1}b$ when $y=b$ namely $x=A^{-1}b$.

\subsection{Prob 4.47}

\subsubsection{a}
This is because for any unspecified diagonal entry, we can
choose an arbitrary large values. By doing this we only need
to consider submatrix composed by all specified diagonla
entries and their corresponding rows and columns. For
example, if one diagonal entry $a_{ii}$ in $A$ is
unspecified, we can assign it to inifinity. Then to satisfy
the condition $A\succeq0$ we only need to consider the
submatrix of $A$ with ith row and ith column removed
$\hat{A}\succeq0$. By doing this we can only consider the
submatrix made by only specified diagonal entries and their
corresponding rows and columns. 


\subsubsection{b}
Suppose $A_j, b_j$ where $j=1,...m$ are all of those
specified entries, we can formulate the problem as:
$$
\text{minimize } s
$$
$$
-A\preceq sI
$$
$$
A_jA=b_j \text{  , where } j=1,\dots m
$$
The problem is feasible when $s\leq0$.

\subsubsection{c}
The optimization problem can be formulated as:
$$
\text{maximize } |A|
$$
$$
-A\preceq 0
$$
$$
A_jA=b_j \text{  , where } j=1,\dots m
$$
Because $A\succeq0$, $-\text{log}det(A)$ is a strictly
convex function over positive definite matrix $A$. The
problem can be reformulated as:
$$
\text{minimize } -logdet(A)
$$
$$
-A\preceq 0
$$
$$
A_jA=b_j \text{  , where } j=1,\dots m
$$
Suppose $S$ is the set of all positive semidefinite
completions of $A$, $S$ is closed, convex and bounded. This
is because all diagonal entries are specified. Therefore,
entries which are not on the diagonal cannot exceed the
maximum diagonal entry otherwise the matrix will not be
positive definite. Therefore, the problem is a convex
optimization problem defined on a convex set and hence has a
unique optimum point.

When $A^*$ is the optimal solution, suppose the element
$a_{ij}$ is unspecified, then the gradient at $a_{ij}$
should be $0$. Therefore we have:
$$
\frac{\partial f_0(A^*)}{\partial a_{ij}} = 2\text{tr}(A^*)^{-1}E_{ij}=0
$$
Because A is symmetric therefore the ijth element as well as
jith element are all 0s. Therefore, $A^*$ has zeros in
all unspecified entries of the origin matrix.


\subsubsection{d}
If there exists a positive definite completion of $A$, then
as section c proved, there exists an unique positive
definite completion $A^*$ for $A$ which maximizes the
determinant. Because as section c proved $A^*$ has zeros in
all unspecified entries of the origin matrix, so
$(A^*)^{-1}$ is tridiagonal.



\subsection{Prob 5.12}
Let $A$ be a matrix whose ith row is $a_i^T$, by introduce
equality constraints $y_i=b_i-a_i^Tx$ we have:
$$
\text{minimize\;} -\sum_i^m\text{log\;}y_i
$$
$$
\text{s.t.\;\;} y=b-Ax
$$
The Lagrangian of the primal problem is:
\begin{align*}
  L(x,y,v) &= -\sum_i^m\text{log\;}y_i + v^T(y-b+Ax)\\
         &= v^TAx -\sum_i^m\text{log\;}y_i + v^Ty-v^Tb
\end{align*}
Therefore, the dual function is:
$$
g(v)=\inf_{x,y} (v^TAx -\sum_i^m\text{log\;}y_i + v^Ty-v^Tb)
$$
When we consider in terms of $y$, this function is unbounded
below if $v\preceq0$. Therefore, the function is bounded
only if $v\succ0$. Because $y_i\geq0$, if we take derivative
in terms of $y$ and set it equals $0$ we have the function
reaches minimum when:
$$
y_i=\frac{1}{v_i}
$$
In terms of $x$ the function is unbounded below if
$v^TA\neq0$. Therefore we have the dual function:
\begin{align*}
g(v)=
\begin{cases}
  \sum_i^m\text{log\;}v_i + m-v^Tb & v^TA=0, v\succ0\\
  -\infty &otherwise
\end{cases}
\end{align*}
Therefore we have the dual problem:
$$
\text{maximize\;\;}\sum_i^m\text{log\;}v_i + m-v^Tb 
$$
$$
\text{s.t.\;\;} v^TA=0\text{,\;} v\succ0
$$


\subsection{Prob 5.31}
Because the problem is convex, every inequality consitraints
are hence convex. $x^*\in R^n$ and $\lambda^*\in R^m$. For any $x$ that is feasible we have:
$$
0\geq f_i(x) \geq f_i(x^*) +\nabla f_i(x^*)^T(x-x^*)\text{
  for all } i=1,\dots,m
$$
Therefore, for all inequality constraints we have:
\begin{align}
  0 &\geq \sum_{i=1}^m\lambda_i^*(f_i(x^*)+\nabla f_i(x^*)^T(x-x^*))\\
\label{eq:1}
&= \sum_{i=1}^m\lambda_i^*f_i(x^*)+\sum_{i=1}^m\lambda_i^*\nabla f_i(x^*)^T(x-x^*)
\end{align}
According to KKT conditions, we have that
$\sum_{i=1}^m\lambda_i^*f_i(x^*)=0$ and
$\sum_{i=1}^m\lambda_i^*\nabla f_i(x^*)^T=-\nabla f_0(x^*)$.
Introducing those two terms into equation~\ref{eq:1} we have:
$$
0\geq-\nabla f_0(x^*)^T(x-x^*)
$$


	\renewcommand\refname{Bibliography}
	\bibliographystyle{ieeetr}
	\bibliography{ass3_ChangLi}
\end{document}

\message{ !name(ass3_ChangLi.tex) !offset(-381) }
