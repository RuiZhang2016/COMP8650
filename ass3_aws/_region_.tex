\message{ !name(ass3_ChangLi.tex)}\documentclass[10pt,a4paper]{article}
\usepackage{amssymb,amsmath}

\begin{document}

\message{ !name(ass3_ChangLi.tex) !offset(-3) }

\title{Ass3 Solutions}
\author{Chang Li}
\maketitle

\section{Solutions}

\subsection{Prob 4.3}
Because $\nabla f(x) = \frac{1}{2}(P+P^T)x + q$,

\begin{align*}
  \nabla f(x^*) &=\frac{1}{2}(
                  \begin{bmatrix}
                    13 & 12 & -2\\
                    12 & 17 & 6\\
                    -2 & 6 & 12
                  \end{bmatrix}
                             +
                             \begin{bmatrix}
                               13 & 12 & -2\\
                               12 & 17 & 6\\
                               -2 & 6 & 12
                             \end{bmatrix})
                                        \begin{bmatrix}
                                          1\\
                                          \frac{1}{2}\\
                                          -1
                                        \end{bmatrix} +
  \begin{bmatrix}
    -22\\
    -14.5\\
    13
  \end{bmatrix}\\
                &=
                  \begin{bmatrix}
                    13 & 12 & -2\\
                    12 & 17 & 6\\
                    -2 & 6 & 12
                  \end{bmatrix}
                             \begin{bmatrix}
                               1\\
                               \frac{1}{2}\\
                               -1
                             \end{bmatrix} +
  \begin{bmatrix}
    -22\\
    -14.5\\
    13
  \end{bmatrix}\\
                &=
                  \begin{bmatrix}
                    -1\\
                    0\\
                    2
                  \end{bmatrix}
\end{align*}
According to equation (4.21)\cite{boyd2004convex}, because for all
$y$ satisfying $y_i \in [-1,1], \; i=1,2,3$, the optimality
condition holds:
\begin{align*}
 \nabla f(x^*)^T(y-x^*)&=\begin{bmatrix}
-1 & 0 & 2
\end{bmatrix}
\begin{bmatrix}
  y_1 - 1\\
  y_2 - 1/2\\
  y_3 + 1
\end{bmatrix}\\
&=
-1(y_1-1)+2(y_3+1)\geq0
\end{align*}
therefore, $x^*$ is optimal for $f(x)$.


\subsection{Prob 4.8}

\subsubsection{a}
The Lagrangian of the primal problem is:
\begin{align*}
  L(x,v) &= c^Tx + v^T(Ax-b)\\
         &= -b^Tv+(c+A^Tv)x
\end{align*}
$L$ is bounded below if and only if $c+A^Tv=0$. Therefore, the
primal problem's optimal value $p^*=-b^Tv$ for some $v$
satisfying $c+A^Tv=0$. The primal problem is unbounded otherwise.

\subsubsection{b}
The problem is always feasible. The Lagrangian of the primal
problem is:
\begin{align*}
  L(x,v) &= c^Tx + \lambda(a^Tx-b)\\
         &= -b\lambda+(c+a^T\lambda)x
\end{align*}
$L$ is bounded below if and only if $c+a^T\lambda=0$ and
$\lambda\geq0$. Therefore, the primal problem has optimal value
$p^*=-b\lambda$ for some $\lambda$ satisfying $c+a^T\lambda=0$ and
$\lambda\geq0$. Otherwise the primal problem is unbounded below.

\subsubsection{c}

The objective function can be written as
$\sum_{i=1}^nc_ix_i$. Therefore, the objective function
reaches minimal when each of component $c_ix_i$ is
minimized subject to $l_i\leq x_i\leq u_i$.

Therefore, for all indices such that $c_i>0$ we have
$x_i=l_i$ and for all indices such that $c_i<0$ we have
$x_i=u_i$. When $c_i=0$, $x_i$ can be any value in the
domain of the problem namely $l_i\leq x_i\leq u_i$.

\subsubsection{d}

The Lagrangian of the primal problem is:
\begin{align*}
  L(x,v,\lambda) &= c^Tx + v(1^Tx-1)-\lambda^Tx\\
         &= -v+(c^T+v-\lambda^T)x
\end{align*}

$L$ is bounded below if and only if $c^T+v-\lambda^T=0$ and
$\lambda\succeq0$, namely $-v< c_i \;\forall i$. Therefore,
the primal problem has optimal value $p^*\geq -v$ for some
$\lambda$ satisfying $\lambda\succeq0$ and for some $v$
satisfying $-v <c_i\;\forall i$. Suppose $c$ has minimal
values $c_{min}$ at indexes $i,j,k\dots$, we can always get
the optimal solution by setting $x_i, x_j, x_k,\dots$ to
$\sum_{m=i,j,k\dots}x_m=1$ at those indexes and $0$ at
anywhere else. Therefore, we have $p^*=c_{min}$. Otherwise
the primal problem is unbounded below.

When we use inequality constraint to replace the equality
constraint, the optimal value is equal to
$p^*=min\{0,c_{min}\}$. This is because we can assign $x$ as
above if $c_{min}<0$ and $x=0$ otherwise.

\subsubsection{e}
Suppose the components of $c$ are sorted in increasing order
as:
$$
c_1\leq c_2\leq \dotsb \leq c_{\alpha} \leq \dotsb \leq c_n
$$
When $\alpha$ is an integer, the primal problem has optimal
value $p^*=\sum_{i=1}^\alpha c_i$, namely the smallest
$\alpha$ elements of $c$ if and only if $x_i=1
\text{\;for\;} i\leq \alpha$ and $x_i=0 \text{\; for \;}
i>\alpha$.

If $\alpha$ is not an integer, let $[\alpha]$ denotes the
nearest integer less than or equal to $\alpha$, the optimal value is
$$
p^*=\sum_{i=1}^{[\alpha]}c_i+c_{1+[\alpha]}(\alpha-[\alpha])
$$
by setting $x_i=1$ for $i\leq [\alpha]$, 
$x_{1+[\alpha]}=\alpha-[\alpha]$ and $x_i=0$ for $i>1+[\alpha]$.

If change the equality constraint to inequality constraint
$1^Tx\leq \alpha$, then the optimal value is 
\begin{align*}
  p^*=
  \begin{cases}
    \sum_{i=1}^{[\alpha]}c_i+c_{1+[\alpha]}(\alpha-[\alpha]) & if c_{1+[\alpha]} <0\\
    \sum_{i=1}^j c_i & if c_j+1\geq =0 \text{\;and\;} j<1+[\alpha]\\
    0 & if c_1>=0
  \end{cases}
\end{align*}
We can always get this by setting:
\begin{align*}
  x_i=
  \begin{cases}
    0 & c_i\geq0\\
    1 & c_i<0
  \end{cases}
\text{\;for\;} i\leq [\alpha]
\end{align*}
\begin{align*}
x_{1+[\alpha]}=
  \begin{cases}
    \alpha-[\alpha] & if c_{1+[\alpha]}<0\\
    0 &\text{otherwise}
  \end{cases}
\end{align*}
 and $x_i=0$ for $i>[\alpha]+1$.

\subsubsection{f}
Let $y_i=d_ix_i$ for $i=1,\dots,n$ and use $y$ to substitute
$x$ in the original problem. Then we have:
$$
\text{minimize} \sum_{i=1}^n \frac{c_i}{d_i}y_i
$$
$$
\text{s.t.\;} 1^Ty=\alpha\text{,\;\;} 0\preceq y \preceq d.
$$
This formulation is similar to section 1.2.5. Suppose the
components of $\frac{c_i}{d_i}$ are sorted in increasing
order as:
$$
\frac{c_1}{d_1}\leq \dotsb \leq \frac{c_\alpha}{d_\alpha} \leq \dotsb \leq\frac{c_n}{d_n} 
$$
Let $k^*=\text{argmax}_kd_1+\dots+d_k\leq \alpha$, the primal problem reaches
optimal value
$p^*=\sum_{i=1}^{k^*}c_i+\frac{c_{k^*+1}}{d_{k^*+1}}(\alpha-(d_1+\dots+d_{k^*}))$
by setting $y_i=d_i$ for $i\leq k^*$, $y_{k^*+1}=\alpha-(d_1+\dots+d_{k^*})$ and
$y_i=0$ for $i>k^*+1$.

Namely by setting $x_i=1$ for $i\leq k^*$, $x_{k^*+1}=\frac{\alpha-(d_1+\dots+d_{k^*})}{d_{k+1}}$ and
$x_i=0$ for $i>k^*+1$.


\subsection{Prob 4.9}



\subsection{Prob 4.47}

\subsection{Prob 5.12}

\subsection{Prob 5.31}


	\renewcommand\refname{Bibliography}
	\bibliographystyle{ieeetr}
	\bibliography{ass3_ChangLi}
\end{document}

\message{ !name(ass3_ChangLi.tex) !offset(-230) }
